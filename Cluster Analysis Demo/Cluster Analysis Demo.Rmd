---
title: "Cluster Analysis In R"
author: "Luke T. Daniels"
date: "4/1/2018"
output: pdf_document
---
Cluster analysis in R is a great tool to find trends and patterns in data. Many people think of statistics as giving definitive answers to questions, however, there are techniques that simply provide further insight into data. Clustering allows users to identify which observations are alike across many different variables. The power of cluster analysis allows us to perform complex analyses that would be near impossible without programs such as R.  

This demonstration serves as an a brief introduction to the statistics behind cluster analysis and the corresponding tools in R.  I will highlight the differences in base `R` and packages such as `factoextra` and `cluster`.  First I will begin my introducing K-means clustering 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\ 

## **Decide on the Clustering Variables**

Cluster analysis has no mechanism for differentiating between relevant and irrelevant variables.  There must be careful consideration for the variables included in the analysis.  There should be signficant differences between the variables. There are select pricipals that should be followed. 
\ 
1.) Avoid using an abundance of variables since this increases the odds that variables are no longer dissimilar.
2.) If there is a high degree of collinearity, specific aspects covered by these variables will be overrepresented. 
3.) Formann (1984) recommends a sample size of at least 2^m^ where m equals the number of clustering variables 

## **Data Preparation**

1.) Rows must be observations and columns must be variables. 
2.) Missing values must be removed or estimated.
3.) The data must be standardized to make variables comparable.
4.) The data used in cluster analysis can be interval, ordinal or categorical 
* There are different methods / calculations that are better for dealing with each type of data type. 

```{r, results='hide'}
data("USArrests")
df <- USArrests
df <- na.omit(df) #Removes any missing values
df.scaled <- scale(df)  #We standardize the data using the function scale()


```
\ 

### **Standardization**

One very important decision that needs to be made involves the scales of the variables being measured.  If one of the variables is measured on a much larger scale than the other variables, then whatever measure is used will be overly influenced by that variable.  For example, if we are looking at the distance between two people based on their IQs and incomes in dollars, the differences in incomes would dominate the distance measurments. We can solve this issue by standardizing the variables! bo


## *Required R Packages*
\ 

1.) **cluster**: Computes clustering algorithms 
\ 


2.) **factorextra**: Used for ggplot2 visualization of clustering results 

```{r results='hide'}
library(ggplot2)
library(factoextra)
library(cluster)
library(latexpdf)
library(purrr)
```

# **Approach #1: Partitioning Methods (K-Means)**
Basically K- mean is an interative process that divides a given data set into K disjoint groups

It starts by placing k centroids randomly in your space. Then you iteraviley do the following: First we run through our data set and for each individual we find the nearest centroid to that individual. To do that, for each $x_i$ you compute the distance between in a $c_j$ (each cluster). This is the Eucleadian Distance. Then you pick the cluster that has the minimum distance of all (nearest cluster). 

Then you assign $x_i$ to that nearest cluster. This process occurs for each individual, so each individual minimizes is distances to the randomly positioned centroid.  Now we need to recompute the centroid by getting the average of all the $x_i$ that was assigned to that cluster. ( All the $x_i$'s that were assigned to the jth cluster and you average them out.) 

At this point we are restricting the analysis to continous variables.  We cannot take the average or distance of categorical variables. 

These are the two basic steps.  You keep running theses steps until no individuals change cluster memberships 


## **Distance Measures**
\ 
K means is implicitly based on pairwise Euclidean distances because the sum of squared deviations from the centroid is equal to the sum of pairwise squared Euclidean distances divided by the Number of Points. A centroid is a multivariate mean in euclidiean spaces.  Euclidean spaces is about euclidean distances. 

The classification of items into groups requires computation od the distance or similarity between each observation. Distance measures defines how similar two elements (x,y) are and will influence the shape of the clusters.

There are many methods to calculating the distance between observations.  The most popular method is Euclidean Distance. An important note is that the *Euclidean* method is often considered the best for dealing with interval data. 


1. Euclidian Distance: 

\[
deuc(x,y)=\sum_i = \ln(x_i - y_i)2d(x,y) = \sqrt(\sum_{i=1}^n(x_1 - y_1)^2)
\]


Other measures, such as the Pearson Equation use correlation based methods.

2. Pearson Correlation Distance

\[
dcor(x,y)= 1-\sum_i = \ln(xi-x)(yi-y)\sum_1= \ln(x1-x)2\sum_i=ln(y_i-y)2d_{cor}(x,y)= 1- \frac{\sum\limits_{i=n}^n(x_i - \bar{x})(y_i - \bar{y})}{\sqrt(\sum\limits_{i=1}^n(x_i-\bar{x}^2\sum\limits_{i=1}^n(y_i-\bar{y}^2)}
\]


A correlation based distance considers objects similar if their features are highly correlated even if the Euclidean distance is not.  If there are observations with the same profiles but with different magnitudes - use correlation-based.  

In a Euclidean analysis, observations with high values will be clustered together. 

Other methods to consider: Manhattan distance, Spearman Correlation, Kendall Correlation 

\ 

## *Caluclating Euclidean Distance in R*
\  


** Function 2 get_dist()

get_dist(x, method = "euclidean", stand = FALSE, ...)
fviz_dist(dist.obj, order = TRUE, show_labels = TRUE, lab_size = NULL,
  gradient = list(low = "red", mid = "white", high = "blue"))

**Function 1 Daisy **
\ 


* The *daisy* function in `cluster` handles different variable types (binary, ordinal, nominal) and computes a distance matrix

\  

* In the daisy function we can indicate which distance measurment equation we would like to use.  "Gower" is best where the data contain non-numeric columns.

\ 

* The metric will be the *Gowers Coefficient*
+ It indicates the % average similarity between all pairs of observations

\ 

*daisy(x, metric = c("euclidean", "manhattan", "gower"), stand = FALSE, type = list(), weights = rep.int(1, p), warnBin = warnType, warnAsym = warnType, warnConst = warnType, warnType = TRUE)*

\ 

The values in the matrix represent the distance between objects. A value of zero indicates that the two items are not different.
\ 

```{r, results = 'hide'}
disteucl <- daisy(df.scaled, metric = "gower", stand= FALSE)
disteucl                                         

euclmatrix <- as.matrix(disteucl)


```
\ 


We have now created a distance matrix! 
 

```{r, results='hide'}
head(euclmatrix)
```

\ 

**Fuction 2 fviz_dist()** 
 
\ 

An easy way to vizualize a distance matrix is to use fviz_dist() from the `factoextra` package

\ 

*fviz_dist(dist.obj, order = TRUE, show_labels = TRUE, lab_size = NULL, gradient = list(low = "red", mid = "white", high = "blue"))*
\ 

The Value indicates the level of dissimilarity.  Zero means the two are exactly similar. 
 
\ 

```{r}
fviz_dist(disteucl, order = TRUE, show_labels = TRUE, lab_size = 4, gradient = list(low = "red", mid = "white", high = "blue"))

```

\ 

## **K-Means Clustering ~ Assigning Clusters**
\ 


## Main Idea: 

\ 

Define clusters so that the total intra-cluster variation is minimized. There a many k-means algorithms available but the most commonly used is the Hartigan-Wong algorithm.  This equation defines the total within cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid
 
\ 

*Total Within Cluster Sum of Squares Equation*

\[
tot.withinss = \sum_k = 1kW(Ck) = \sum_k=1k\sum_x\epsilon Ck(x_i -\mu k )2 tot.withinss \sum\limits_{k=1}^k W(C_k)=\sum\limits_{k=1}^k\sum\limits_{x_i\in C_k}(x_ - \mu_k)^2
\]
 

Each observation of xi_k is assigned to a cluster so that the sum of squares distance of the observation to their assigned cluster centers mu_k is minimized
This equation measures the compactness of the clustering - we want it to be minimized.
 
\ 

**The Hartigon Wong algorithm requires the user to estimate the value of k clusters.** 
\  

### Hartigon Wong Equation In Action!
 
\ 
kmeans(x, centers, iter.max = 10, nstart = 1,
       algorithm = c("Hartigan-Wong", "Lloyd", "Forgy",
                     "MacQueen"), trace=FALSE)
                     
* X = The scaled data set 
* Centers = The Number of K clusters
* iter.max = The number of iterations to be run.  Should be relatively high 
* nstart = R will try 25 different random starting starting assignments and then select the best results corresponding to the on within cluster variation. (Strongly recomended 25 - 50)

```{r, results='hide'}
kmeans(df.scaled, centers = 3, iter.max = 10, nstart = 1)
```

The algorithm starts by selecting k objects from the data set to serve as the intitial centers.  These are known as centroids. Each object is then assigned to the closes centroid defined by the Euclidean distance.  After this assignment, the algorithm computes the new mean value of each cluster. Every observation is checked again to see if it is actually closer to another cluster. This occurs until the cluster assignments stop changing. 
\  

The main problem is that if we choose the wrong k, then the whole analysis is wrong! Human error! 
 
\ 

## **How Do We Choose the Right Number of K Clusters?**
 
\ 

There are three methods to determine the optimal clusters:

1.) Elbow Method
2.) Silhouette Method 
3.) Gap Statistic. 

\ 

### **Elbow Method**

The idea is to compute k-means clustering using different values of clusters k. Then the Within Sum of Squares is drawn according to the number of clusters. 
 
\ 


The Elbow method analyzes the percentage of variance explained as a function of clusters.  If we plot the percentage of variance explained by the clusters against the number of clusters, the first cluster will explain a lot but this will begin to drop to where it explain less and less. 

Clusters must be defined so that the total within cluster variation is minimized 
 
\[
minimize(\sum\limits_{k=1}^kW(C_k)
\]
is the kth cluster and  is the within cluster variation. 


#### The Elbow Method Using Base R 
\ 


```{r}
set.seed(123)
# function to compute the total within-cluster sum of squares
k.max <- 15 
wss <- sapply(1:k.max, function(k){kmeans(df,k, nstart = 50)$tot.withinss})
wss
plot(1:k.max, wss, type = "b", pch= 19, frame = FALSE, 
     xlab = "Number of Clusters K", 
     ylab = "Total within-custers sum of squares")


```

\ 

#### The Elbow Method in Factoextra
\ 

We rely on the 'factoextra' package once again. The "bend in the knee" in the graph below is considered to be the appropriate number of clusters 

```{r}
fviz_nbclust(df.scaled, kmeans, method = "wss")
```

```{r}
fviz_nbclust(df.scaled, kmeans, method = "wss") + 
  geom_vline(xintercept = 4, linetype = 3)
```

\ 


### ** Silhouette Method**
 
\ 

This approach measures the quality of clusters. It determines how well each object lies within the cluster. This algorithm computes the average silhouette of observations for different values of k.  The optimal number of cluster k is the one that maximizes the average silhouette over a range of possible values 
 
\ 

#### In base R

\ 

```{r}
#Function to compute the average silhouette for k cluseter


#avg_sil <- function(k){ 
#  km.res <- kmeans(df, centers = k, nstart = 25)
 #ss <- silhouette(km.res$cluster, dist(df))
 # mean(ss[, 3])
#}

# Compute and plot wss for k = 2 to k =15
k.values <- 2:15

#extracting silhouettes
#avg_sil_values <- map_dbl(k.values, avg_sil)

#plot(k.values, avg_sil_values,
#type = "b", pch = 19, frame = FALSE, 
#xlab = "Number of Cluster K",
#ylab = "Average Silhouettes")
```

There will be an easier way of calculating silhouettes later in this demonstration!
\ 
 

## **Computing the Means with K = 4 Clusters**

\ 

Remember, using the k-means approach, the center of a cluster is defined as being the average of all the points within a cluster!

```{r}
set.seed(100)  # Since the alogorithm starts with k randomly selected centers we must set the seed 
km.res <- kmeans(df, 4, nstart = 50) # R will try 50 different random starting assignments and then select the result corresponding to the lowest within cluster variation 
km.res
```
\ 

Now we have classified each unit by their respective cluster.  This model has a good fit (90.2%)

Now that each unit has been assigned a cluster, we can calculate the cluster means 
\ 

```{r}
aggregate(USArrests, by=list(cluster= km.res$cluster),mean)
```
\ 

We can add the cluster assignments to the original data frame

```{r}
df <- cbind(USArrests, cluster = km.res$cluster)
head(df)
```
\ 

Finally, we can vizualize

```{r}
fviz_cluster( km.res, data = df, palette = c("Red", "Blue", "Green", "Purple"), 
              ellipse.type = "euclid", star.plot = TRUE, 
              repel = TRUE, 
              ggtheme = theme_minimal())
```

\ 

## THE GAP APPROACH 

The Gap Statistic Compares the Total Within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be the value that maximizes the gap statistic.  This means that the clustering strucutre is far away from the random uniform distribution of points. 

The algorithm works by: 
1.) Clustering the observed data by k = 1 - $k_{max}$ and compues the within intra cluster
2.) Then it generates a random uniform distribution of data and clusters the random data by k = 1 - $k_{max}$
3.) Computes the estimated gap statistic as the deviation of observed WSS value from its expected value. 4.) It chooses the number of clusters as the smallest value of k such that the gap statistic is within one standard deviation of the gap at k + 1


The disadvantage of elbow and average silhouette methods is that they measure a global clustering characteristic only.  A more sophisticated method is to use the gap statistic which provides a statistical procedure to formalize the other two.

# **The PAM Approach**
\ 

In the K-mean approach above, the center of a cluster is calculated as the mean value of all the data points in a cluster.  This makes the K-means approach very sensitive to outliers and noise. 

The K-mediod approach using the PAM algorithm is more robust and therefore less sensitive to outliers.  A mediod is an object within a cluster for which the average dissimilarity between it and all the other objects in a cluster is minimal. (The most centrally located point).  This point is representative of the cluster and less skewed by outliers. 

1.) **Estimate K- Clusters Using Silohette Method in "factoextra"**

The idea of using the silhoutte method is to compute PAM using different values of clusters k. 

To estimate the optimal number of clusters, we'll use the average silhouette method. The idea is to compute PAM algorithm using different values of clusters k. Next, the average clusters silhouette is drawn according to the number of clusters. The average silhouette measures the quality of a clustering. A high average silhouette width indicates a good clustering. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k.


```{r}
fviz_nbclust( df, pam, method = "silhouette") + theme_classic()

```

* The maximum silhouette width occurs at k=2, suggesting that the optimal number of clusters is 2! 


### Computing the PAM Clustering

**Function 3 pam()**

*pam(x, k, metric = "euclidean", stand = FALSE)*
* x is the disimilarity matrix created from the daisy() function 
* k is the number of clusters 
* stand is logical value, if true the columns in x were standardized before calculating disimilarities. This is ignored when x is a disimilarity matrix.

```{r}
pam.res <- pam(df, 2)
pam.res
```

\ 

The pam() function returns the mediod objects!  To access these:

```{r}
pam.res$medoids
```


Finally we can plot the 2 new clusters! 

```{r}
fviz_cluster( pam.res, palette = c("blue", "red"), 
              ellipse.type = "t", 
              repel = TRUE,  # Avoids overplotting labels
              ggtheme = theme_classic() )
              
```
\ 

## **Validating Our Results**

```{r}
library(fpc)
hc.res <- eclust(df, "hclust", k= 6 , hc_metric = "euclidean",
                 hc_method = "ward.D2", graph = FALSE)
fviz_dend(hc.res, show_labels = TRUE, palette = "jco", repel = TRUE, cex = .5,as.ggplot =TRUE) + 
  theme(axis.text.x=element_text(size = 10 , angle=90))

km.res <- eclust(df, "kmeans", k = 6, nstart = 25, graph = FALSE)
fviz_cluster(km.res, geom = "point", ellipse.type = "norm", ggtheme = theme_minimal())


fviz_silhouette(km.res, palette = "jco", ggtheme = theme_classic())


```

The silhouette coefficient measures how similar an object is to the other objects in its own cluster versus those in the neighbot cluster. 
\ 

* A value close to 1 indicates that the object is well clustered. In other words, the obkect is similar to the other objects in its group. 

* A value close to -1 indicates that the object is poorly clustered, and that assignment to some other cluster would improve the overal results. 

* We can see that cluster 4 has some negative values which means those observations are not clustered correctly. 

We can find the names of these samples and determine the clusters as the following 

```{r}
# Extracting silhouette values 

silinfo <- km.res$silinfo
silinfo

silinfo$clus.avg.widths

# Determining the bad observations 
km.res$silinfo$widths # Look at the width of each observation 
sil <- km.res$silinfo$widths
sil
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]
```





# **Partitioning Clustering: PAM Approach**

Recall that in k-means clustering, the ceter of a given cluster is calculated as the mean value of all the data points in the cluster.  Using this method introduces the potential for large outliers to drastically influence the cluster mean.  
\ 
We can solve this using the k-mediod algorithm.  A mediod refers to an object within a cluster for which the average dissimilarity between it and all the other points of the cluster is minimal.  In other words, it is the most centrally located point in the cluster.  Therefore it does not rely on taking the average.  This means that, the algorithm is less sensitive to noise and outliers. 

\ 

We have also been using euclidean distances in our clusters.  If your data contains outliers Manhattan distaances will give a more robust result. While Base R can calculate the PAM approach, the cluster package is much simpliler. 



# **Hierarchical Clustering**

In contrast to partitioning clustering, hierarchical clustering does not required to pre-specify the number of clusters. 

Agglomerative clusting (AGNES): Each observation is initially considered as a cluster of its own leaf. Then the most similar clusters are successively merged. In this way, agglomerative clustering works in a "bottom up" fashion. 

Steps: 
1.) Compute disimilarity between every pair of objects
2.) Based on the disimilarity use algorithm to cluster
3.) Determine where to cut the hierarchical tree into clusters.

* The data should be numeric!





## **Finding the P-Values**

```{r}
library(pvclust)
data("lung")
head(lung)
View(lung)

set.seed(123)
ss <- sample(1:73, 30) #extract 20 samples out of
newdata <- lung[, ss]
res.pv <- parPvclust(cl = NULL,newdata, method.hclust = "average", method.dist = "correlation", nboot =1000, iseed = NULL)

plot(res.pv, hang = -1, cex = 0.5)
```

Values on the dendrogram are the Approximately Unbiased P-Values(Red,left).  These are similar to bootstrap values but are more technical.  The Bootstrap P-values are more commonly known (green, right).  In this case, pvclust will bootstrap the columns of the dataset.  An important note is that if you would like to bootstrop the rows, you can transpose the data.

For example a BP value of 55 indicates that these 8 genes ended up in the same cluster 55 runs out of the 100 bootstraps. If the BP is high then the cluster can be supported by the data. 

